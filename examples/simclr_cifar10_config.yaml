# Complete SimCLR training configuration for CIFAR-10
# This example shows how to configure all components via Hydra
#
# Usage:
#   python -m stable_pretraining.train --config-path ../examples --config-name simclr_cifar10_config
#
# With overrides:
#   python -m stable_pretraining.train --config-path ../examples --config-name simclr_cifar10_config \
#       module.optimizer.lr=0.01 \
#       trainer.max_epochs=200

# Random seed for reproducibility
seed: 42

# PyTorch Lightning Trainer configuration
trainer:
  _target_: lightning.Trainer
  max_epochs: 1000
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  num_sanity_val_steps: 0
  enable_checkpointing: false

  # Callbacks configuration - showing various callback examples
  callbacks:
    # Online linear probe for evaluation during training
    - _target_: stable_pretraining.callbacks.OnlineProbe
      name: linear_probe
      input: embedding
      target: label
      probe:
        _target_: torch.nn.Linear
        in_features: 512
        out_features: 10
      loss_fn:
        _target_: torch.nn.CrossEntropyLoss
      metrics:
        top1:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: 10
        top5:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: 10
          top_k: 5

    # KNN probe for non-parametric evaluation
    - _target_: stable_pretraining.callbacks.OnlineKNN
      name: knn_probe
      input: embedding
      target: label
      queue_length: 20000
      input_dim: 512
      k: 10
      metrics:
        accuracy:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: 10

    # LiDAR callback for monitoring representation quality
    - _target_: stable_pretraining.callbacks.LiDAR
      name: lidar
      input: embedding
      n: 128

    # RankMe callback for measuring feature diversity
    - _target_: stable_pretraining.callbacks.RankMe
      name: rankme
      input: embedding
      epsilon: 0.01

  # Logger configuration (optional - can use WandB, TensorBoard, etc.)
  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
    entity: stable-ssl
    project: cifar10-simclr
    log_model: false
    name: simclr-resnet18

# Module configuration - the main training module
module:
  _target_: stable_pretraining.Module

  # Forward function for the training loop
  forward:
    _target_: stable_pretraining.forward_functions.simclr_forward

  # Backbone network
  backbone:
    _target_: stable_pretraining.backbone.from_torchvision
    name: resnet18
    low_resolution: true

  # Projection head for SSL
  projector:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: torch.nn.Linear
        in_features: 512
        out_features: 2048
      - _target_: torch.nn.BatchNorm1d
        num_features: 2048
      - _target_: torch.nn.ReLU
        inplace: true
      - _target_: torch.nn.Linear
        in_features: 2048
        out_features: 2048
      - _target_: torch.nn.BatchNorm1d
        num_features: 2048
      - _target_: torch.nn.ReLU
        inplace: true
      - _target_: torch.nn.Linear
        in_features: 2048
        out_features: 128

  # Loss function
  simclr_loss:
    _target_: stable_pretraining.losses.NTXEntLoss
    temperature: 0.5

  # Optimizer configuration
  optim:
    optimizer:
      type: LARS
      lr: 5.0
      weight_decay: 1e-6
    scheduler:
      type: LinearWarmupCosineAnnealing
      warmup_epochs: 10
    interval: epoch

# Data module configuration
data:
  _target_: stable_pretraining.data.DataModule

  # Training dataloader
  train:
    _target_: torch.utils.data.DataLoader
    batch_size: 256
    num_workers: 8
    drop_last: true
    sampler:
      _target_: stable_pretraining.data.sampler.RepeatedRandomSampler
      n_views: 2
      dataset:  # This will be filled by the dataset below
        _target_: stable_pretraining.data.FromTorchDataset
        names: ["image", "label"]
        dataset:
          _target_: torchvision.datasets.CIFAR10
          root: ~/data
          train: true
          download: true
        transform:
          _target_: stable_pretraining.data.transforms.MultiViewTransform
          transforms:
            # First augmentation view
            - _target_: stable_pretraining.data.transforms.Compose
              transforms:
                - _target_: stable_pretraining.data.transforms.RGB
                - _target_: stable_pretraining.data.transforms.RandomResizedCrop
                  size: [32, 32]
                  scale: [0.2, 1.0]
                - _target_: stable_pretraining.data.transforms.RandomHorizontalFlip
                  p: 0.5
                - _target_: stable_pretraining.data.transforms.ColorJitter
                  brightness: 0.4
                  contrast: 0.4
                  saturation: 0.2
                  hue: 0.1
                  p: 0.8
                - _target_: stable_pretraining.data.transforms.RandomGrayscale
                  p: 0.2
                - _target_: stable_pretraining.data.transforms.ToImage
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
            # Second augmentation view (slightly different)
            - _target_: stable_pretraining.data.transforms.Compose
              transforms:
                - _target_: stable_pretraining.data.transforms.RGB
                - _target_: stable_pretraining.data.transforms.RandomResizedCrop
                  size: [32, 32]
                  scale: [0.08, 1.0]
                - _target_: stable_pretraining.data.transforms.RandomHorizontalFlip
                  p: 0.5
                - _target_: stable_pretraining.data.transforms.ColorJitter
                  brightness: 0.4
                  contrast: 0.4
                  saturation: 0.2
                  hue: 0.1
                  p: 0.8
                - _target_: stable_pretraining.data.transforms.RandomGrayscale
                  p: 0.2
                - _target_: stable_pretraining.data.transforms.RandomSolarize
                  threshold: 0.5
                  p: 0.2
                - _target_: stable_pretraining.data.transforms.ToImage
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
    dataset:  # Reference to the dataset defined above
      _target_: stable_pretraining.data.FromTorchDataset
      names: ["image", "label"]
      dataset:
        _target_: torchvision.datasets.CIFAR10
        root: ~/data
        train: true
        download: true
      transform: ${data.train.sampler.dataset.transform}

  # Validation dataloader
  val:
    _target_: torch.utils.data.DataLoader
    batch_size: 256
    num_workers: 10
    dataset:
      _target_: stable_pretraining.data.FromTorchDataset
      names: ["image", "label"]
      dataset:
        _target_: torchvision.datasets.CIFAR10
        root: ~/data
        train: false
        download: true
      transform:
        _target_: stable_pretraining.data.transforms.Compose
        transforms:
          - _target_: stable_pretraining.data.transforms.RGB
          - _target_: stable_pretraining.data.transforms.Resize
            size: [32, 32]
          - _target_: stable_pretraining.data.transforms.ToImage
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]

# Hydra configuration (optional)
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: false
